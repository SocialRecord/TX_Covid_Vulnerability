import numpy as np
import pandas as pd
import datetime

# ---------------------#
#  Acquire Functions  #
# ---------------------#


def acquire_people_data():
    """
    Read in the People csv, filter to only Texas counties,
    drop unecessary columns, return dataframe
    """
    df = pd.read_csv("./Data/People.csv")
    # Restrict df to only Texas
    df = df[df.State == "TX"]
    # Keep only the columns we want and set index to FIPS
    good_columns = [
        "FIPS",
        "LandAreaSQMiles2010",
        # Population estimates
        "TotalPopEst2018",
        "PopDensity2010",
        "Age65AndOlderNum2010",
        "Under18Num2010",
        "NaturalChange1018",
        # Household size
        "AvgHHSize",
        "TotalOccHU",
        "OwnHomeNum",
        # Head of Household
        "FemaleHHNum",
        "TotalHH",
        "NonEnglishHHNum",
        "HH65PlusAloneNum",
        # Education columns
        "Ed1LessThanHSNum",
        "Ed2HSDiplomaOnlyNum",
        "Ed3SomeCollegeNum",
        "Ed4AssocDegreeNum",
        "Ed5CollegePlusNum",
        # Race columns
        "WhiteNonHispanicNum2010",
        "BlackNonHispanicNum2010",
        "AsianNonHispanicNum2010",
        "NativeAmericanNonHispanicNum2010",
        "HispanicNum2010",
        "MultipleRaceNum2010",
    ]
    df = df[good_columns]
    return df


def acquire_veteran_data():
    """
    Read in Veteran csv, filter to only Texas counties, 
    drop unecessary columns, return dataframe
    """
    df = pd.read_csv("./Data/Veterans.csv")
    # Restrict df to only Texas
    df = df[df.State == "TX"]
    # Keep only the columns we want and set index to FIPS
    good_columns = [
        "FIPS",
        "State",
        # Income for both vets and non-vets
        "MedianVetsInc",
        "MedianNonVetsInc",
        # Number of vets 18 and over
        "Vets18ONum",
        # Number of vets in civilian labor force
        "CLFVets18to64Num",
        "NonVetsDisabilty",
        "VetsDisabilty",
        "NonVetsPoor",
        "VetsPoor",
    ]
    df = df[good_columns]
    return df


def acquire_covid_cases():
    """
    Takes in the case counts from the Dept. of State Health Services official website.
    This file was precleaned in Excel before importing into our notebook; 
    if you update this file you will need to do these precleaning steps:
    We deleted the original column names and turned them into actual dates, 
    added columns where they were missing data and filled them with zeros to have every day accounted for,
    deleted the counties reporting row, and deleted the notes and disclaimers.
    Creates a new df with each county, population, number of covid cases, and the pct of the population infected. 
    """
    df = pd.read_excel("./Data/Texas_COVID-19_Case_Count_Data_by_County.xlsx")
    # Keep only most recent covid data
    columns_to_keep = [
        "County Name",
        "Population",
        datetime.datetime(
            2020, 6, 25, 0, 0
        ),  # if this file gets updated, this column should be replaced with the most recent date
    ]
    df = df[columns_to_keep]
    # rename last column
    df = df.rename(
        columns={
            "County Name": "County",
            "Population": "population",
            datetime.datetime(2020, 6, 25, 0, 0): "num_covid_cases",
        }
    )
    # create infection rate column
    df["infection_pct"] = (df["num_covid_cases"] / df["population"])
    # drop nans
    df = df.dropna()
    # lowercase the County column
    df["County"] = df["County"].str.lower()
    return df


def acquire_income_data():
    """
    Read in the Income csv, filter to only Texas counties,
    drop unecessary columns, return dataframe
    """
    # Read in csv and set inded to FIPS
    df = pd.read_csv("./Data/Income.csv")

    # Create a boolean mask where only observations where TX is the state are counted as true
    # Then apply the mask to the whole dataframe
    mask = df["State"] == "TX"
    df = df[mask]

    # Drop columns we don't need
    df = df.drop(
        columns=[
            "PerCapitaInc",
            "PovertyUnder18Pct",
            "PovertyAllAgesPct",
            "Deep_Pov_Children",
            "State",
        ]
    )

    # Lowercase the county names
    df["County"] = df["County"].str.lower()

    return df


def acquire_food_data():
    """
    Read in the Food1 excel file, filter to only Texas counties,
    drop unecessary columns, return dataframe
    """

    # Pull in only the columns I want
    file_loc = "./Data/Food1.xlsx"
    df = pd.read_excel(file_loc, index_col="FIPS", na_values=["NA"], usecols="A,B,E")

    # Restrict df to Texas only
    mask = df["State"] == "TX"
    df = df[mask]

    # Drop the state column
    df = df.drop(columns="State")
    return df


def acquire_jobs_data():
    """
    Read in the jobs csv, filter to only Texas counties,
    drop unecessary columns, return dataframe
    """
    df = pd.read_csv("./Data/Jobs.csv")
    # Restrict df to only Texas
    df = df[df.State == "TX"]
    # Keep only the columns we want and set index to FIPS
    good_columns = [
        "FIPS",
        # unemployment rate in 2018
        "UnempRate2018",
        "NumUnemployed2018",
        "NumEmployed2018",
        "NumCivLaborforce2018",
        # percentage of workforce in various working industries
        "PctEmpAgriculture",
        "PctEmpMining",
        "PctEmpConstruction",
        "PctEmpManufacturing",
        "PctEmpTrade",
        "PctEmpTrans",
        "PctEmpInformation",
        "PctEmpFIRE",
        "PctEmpServices",
        "PctEmpGovt",
    ]
    df = df[good_columns]
    return df


def acquire_tx_county_class():
    """
    Read in the tx_county_class csv, filter to only Texas counties,
    drop unecessary columns, return dataframe
    """
    county_class = pd.read_csv("./Data/County Classifications.csv")

    # get only TX datapoints
    mask = county_class.State == "TX"
    tx_county_class = county_class[mask]

    # rename the FIPS column
    tx_county_class.rename(columns={"FIPStxt": "FIPS"}, inplace=True)

    columns_to_drop = [
        "RuralUrbanContinuumCode2003",
        "UrbanInfluenceCode2003",
        "PersistentChildPoverty_1980_2011",
        "Perpov_1980_0711",
        "Hipov",
        "Metro2003",
        "NonmetroNotAdj2003",
        "NonmetroAdj2003",
        "Micropolitan2003",
        "FarmDependent2003",
        "ManufacturingDependent2000",
        "LowEducation2000",
        "RetirementDestination2000",
        "PersistentPoverty2000",
        "PersistentChildPoverty2004",
        "RecreationDependent2000",
        "Nonmetro2013",
        "Gas_Change",
        "Oil_Change",
        "Type_2015_Nonspecialized_NO",
        "Low_Education_2015_update",
        "HiCreativeClass2000",
        "EconomicDependence2000",
        "Nonmetro2003",
        "Noncore2013",
        "Oil_Gas_Change",
        "Noncore2003",
        "State",
        "County",
        "Type_2015_Update",
    ]

    # drop the columns
    tx_county_class = tx_county_class.drop(columns=columns_to_drop)

    return tx_county_class


def acquire_testing_totals():
    df = pd.read_excel("./Data/Cumulative Tests over Time by County.xlsx", header=1)
    df = df.dropna(thresh=2).drop(index=[254, 255, 256])
    df = df[["County", "Tests Through June 16"]]
    df = df.rename(columns={"Tests Through June 16": "total_tests"})
    df["total_tests"] = df["total_tests"].astype("int")
    df["County"] = df["County"].str.lower()
    return df


# ------------------------------------------#
#  Acquire Hospital & Nursing Home Totals  #
# ------------------------------------------#


def get_texas_hospitals():
    """Uses hospitals csv and return just the open texas hospitals"""
    hospitals = pd.read_csv("./Data/Hospitals.csv")

    # create mask to filter for hospitals in TX
    mask = hospitals.STATE == "TX"
    tx_hospitals = hospitals[mask]

    # drop useless columns
    tx_hospitals = tx_hospitals.drop(
        columns=[
            "X",
            "Y",
            "FID",
            "ID",
            "ZIP4",
            "TELEPHONE",
            "POPULATION",
            "COUNTRY",
            "NAICS_CODE",
            "SOURCE",
            "SOURCEDATE",
            "VAL_METHOD",
            "VAL_DATE",
            "WEBSITE",
            "ALT_NAME",
            "TTL_STAFF",
            "TRAUMA",
            "HELIPAD",
        ]
    )

    # rename county_fips to fips
    tx_hospitals.rename(columns={"COUNTYFIPS": "FIPS"}, inplace=True)

    # drop the closed hospitlas
    mask = tx_hospitals.STATUS == "OPEN"
    tx_hospitals = tx_hospitals[mask]

    return tx_hospitals


def create_hospital_count_df():
    """
    Uses the get_texas_hospitals function and creates a dataframe
    totaling the number of hospitals homes per county
    """
    # Bring in the hospitals dataframe
    hosp = get_texas_hospitals()

    # Get the value counts for each FIPS (county)
    df = pd.DataFrame(hosp.FIPS.value_counts()).reset_index()

    # Rename the columns
    df.rename(columns={"FIPS": "num_hospitals"}, inplace=True)
    df.rename(columns={"index": "FIPS"}, inplace=True)

    df["FIPS"] = df.FIPS.astype(int)

    return df

def create_bed_count_df():
    """
    Uses the get_texas_hospitals function and creates a dataframe
    totaling the number of hospitals homes per county
    """
    # Bring in the hospitals dataframe
    df = get_texas_hospitals()

    # Get just the bed count and fips
    columns_to_keep = ['FIPS', 'BEDS']

    df = df[columns_to_keep]
    
    #reset index
    df = df.reset_index()
    
    #Change the data types
    df["FIPS"] = df.FIPS.astype(int)
    df['BEDS'] = df.BEDS.astype(int)
    
    # Replace megatives 
    df.BEDS.replace(-999, 0, inplace = True)
    
    # Make into data frame
    df = pd.DataFrame(df.groupby('FIPS').BEDS.sum()).reset_index()
    
    
    
    return df

def acquire_nh_data():
    """
    Read in the NurstinFacilities excel file, filter to only Texas counties,
    drop unecessary columns, return dataframe
    """
    df = pd.read_excel("./Data/NursingFacilities.xlsx", header=1)
    # Keep only relevant columns
    columns_to_keep = [
        "Facility Name",
        "County_",
        "Service  Type",
        "Facility Certified",
        "Physical Address",
        "Physical Address CITY",
        "Physical Address Zipcode",
        "Total Licensed Capacity",
        "Alzheimer Capacity",
        "ICFIID Beds",
    ]
    df = df[columns_to_keep]
    # Rename columns
    df = df.rename(
        columns={
            "Facility Name": "NH_name",
            "County_": "county",
            "Service  Type": "type_of_service",
            "Facility Certified": "is_certified",
            "Physical Address": "address",
            "Physical Address CITY": "city",
            "Physical Address Zipcode": "zip",
            "Total Licensed Capacity": "total_capacity",
            "Alzheimer Capacity": "alz_capacity",
            "ICFIID Beds": "ICU_beds",
        }
    )
    # Fill na values based on zip codes
    df.loc[(df["county"].isnull()) & (df["zip"] == "78613"), "county"] = "WILLIAMSON"
    df.loc[(df["county"].isna()) & (df["zip"] == "76504"), "county"] = "BELL"
    df.loc[(df["county"].isna()) & (df["zip"] == "76802"), "county"] = "BROWN"
    # delete any characters past the first 5 letters of the zipcode
    df["zip"] = df["zip"].str[:5]
    # make zip column int
    df = df.astype({"zip": "int"})
    return df


def create_county_fips_dataframe():
    """
    returns a dataframe with all county names and fips number
    """
    # bring in fips data
    fips = acquire_income_data()

    cols_to_keep = ["FIPS", "County"]

    # remove useless columns
    fips = fips[cols_to_keep]

    # lowercase the County names
    fips["County"] = fips["County"].str.lower()

    return fips


def create_nursing_home_counts_df():
    """
    Uses the acquire_nh_data function and creates a dataframe
    totaling the number of nursing homes per county
    """
    # Bring in the nursing home dataframe
    nh = acquire_nh_data()

    # Get the value counts for each county and create df
    df = pd.DataFrame(nh.county.value_counts()).reset_index()

    # Rename columns
    df.rename(columns={"county": "num_nursing_homes"}, inplace=True)
    df.rename(columns={"index": "County"}, inplace=True)

    # make county name all lower
    df["County"] = df["County"].str.lower()

    # sort the df counties
    df = df.sort_values(by="County").reset_index()
    df = df.drop(columns="index")

    # bring in fips df
    fips = create_county_fips_dataframe()

    # merge fips df with nuring home df
    fips_nh = fips.merge(df, left_on="County", right_on="County", how="outer")

    return fips_nh


# ----------------------------------------#
#  Acquire Covid Cases Time Series Data  #
# ----------------------------------------#


def acquire_covid19_cases_data():
    # read covid19 cases data
    df = pd.read_excel("./Data/Texas_COVID-19_Case_Count_Data_by_County.xlsx")
    # drop defaulted null values from excel
    df = df.dropna()
    df = df.drop(["Population"], axis=1)
    df = df.set_index("County")
    # transpose data
    df = df.T
    # calculate time difference
    df = df.diff()
    df = df.dropna()
    return df


def acquire_testing_timeseries_data():
    df = pd.read_excel("./Data/Cumulative Tests over Time by County.xlsx", header=1)
    df = df.dropna(thresh=2).drop(index=[254, 255, 256])
    cols = pd.date_range(start="2020-04-21", end="2020-06-16")
    cols = cols.insert(0, "County")
    df.columns = cols
    df = df.set_index("County")
    df = df.T
    df = df.replace("--", 0).replace("-", 0).astype("int")
    df[df < 0] = 0
    return df

def create_melted_time_series_df():
    """ 
    Time_series.xlsx is the most recent excel file from https://www.dshs.state.tx.us/coronavirus/TexasCOVID19DailyCountyCaseCountData.xlsx
    This file needs to be cleaned up first by deleting the last few rows with notes, first three rows, and creating a new header with fixed dates.
    This function also uses certain columns from the mother_frame, these columns can be changed as desired by adding/deleting them from the columns_to_keep variable - you just need to ensure to also update the id_vars variable in the melt function.
    """
    df = pd.read_excel("./Data/Texas_COVID-19_Case_Count_Data_by_County.xlsx")
    # add columns from motherframe
    mf = pd.read_csv("./Data/mother_frame.csv")
    columns_to_keep = [
        "infection_pct",
        "pop_density_category",
        "infection_pct_category",
    ]
    mf = mf[columns_to_keep]

    # first create new daily cases df from the edited time series file
    daily_df = df.drop(columns="Population")
    # set index to the county for transpose
    daily_df = daily_df.set_index("County")
    daily_df = daily_df.T
    # Calculate daily difference in case counts
    daily_df = daily_df.diff()
    # Transpose back
    daily_df = daily_df.T
    # Fill NaNs with 0, replace negatives with 0
    daily_df = daily_df.fillna(0).clip(lower=0)
    daily_df = daily_df.reset_index()
    # Melt daily_df so that each row is a single day in a single county
    melted = pd.melt(
        daily_df, id_vars=["County"], var_name="date", value_name="daily_cases"
    )

    # Create cumulative cases over time df by adding the descriptive columns from the mf to the edited time series file
    c_df = pd.concat([df, mf], axis=1)
    # Melt c_df so that each row is a single day in a single county
    melted_c = pd.melt(
        c_df,
        id_vars=[
            "County",
            "Population",
            "infection_pct",
            "pop_density_category",
            "infection_pct_category",
        ],
        var_name="date",
        value_name="cases",
    )
    # Add the daily cases from the first melt to the cumulative melted_c df
    melted_c = pd.concat([melted_c, melted.daily_cases], axis=1)
    melted_c.to_csv('./Data/melted_df.csv')
    
    return melted_c


# ----------------------------------------#
#  Acquire Texas Region ID #              #
# ----------------------------------------#

def acquire_health_region_id():
    """
    Acquires the health region data and prepares the dataframe to merge
    onto the main dataframe
    """
    #read in the file
    df_region = pd.read_excel("./Data/PHR_MSA_County_masterlist.xlsx")
    
    #list coloumns to drop
    col_to_drop = ['FIPS #', 
               'County #', 
               'Metropolitan Statistical Area (MSA)',
               'Metropolitan Divisions (MD)',
               'Metro Area (82)',
               'NCHS Urban Rural Classification (2006)',
               'NCHS Urban Rural Classification (2013)',
               'Border 32 (La Paz Agreement)',
               'Border 15']
    
    #drop columns
    df_region.drop(columns = col_to_drop, inplace = True)
    
    #drop the na rows
    df_region.dropna(inplace = True)
    
    #rename columns
    df_region.rename(columns = {"County Name": "County"}, inplace = True)
    
    #make county names all lower case
    df_region.County = df_region.County.str.lower()
    
    #bring in fips numbers
    fips = create_county_fips_dataframe()
    
    #merge fips onto region id df
    df_region_fips = fips.merge(df_region, left_on="County", 
                                right_on="County", how="outer")
    
    #drop county column
    df_region_fips.drop(columns = "County", inplace = True)
    
    #fill TX row with 0 for region ID
    df_region_fips.fillna(0, inplace= True)
    
    #set the datatype for region ID to an int
    df_region_fips["Public Health Region (11)"] = df_region_fips["Public Health Region (11)"].astype("int")
    
    return df_region_fips

# ----------------------#
#  Acquire SVI from CDC #
# ----------------------#

def acquire_cdc_svi_index():
    """
    Acquires the Texas counties csv from the CDC fir info
    on the Social Vilnerability Index (SVI) and prepares the dataframe to merge
    onto the main dataframe
    """
    #read in the dataframe
    tx_svi = pd.read_csv("./data/Texas_COUNTY.csv")
    
    #columns to keep
    col_to_keep = ['FIPS',
              'RPL_THEME1',
              'RPL_THEME2',
              'RPL_THEME3',
              'RPL_THEME4',
              'RPL_THEMES']
    
    #drop columns that are not needed
    tx_svi = tx_svi[col_to_keep]
    
    return tx_svi


# ---------------------#
#  Create MotherFrame  #
# ---------------------#


def merge_dataframes():
    """
    This function creates dataframes from each csv and excel files
    in the Data folder and merges them together
    """
    # create each dataframe
    food = acquire_food_data()
    income = acquire_income_data()
    people = acquire_people_data()
    veteran = acquire_veteran_data()
    jobs = acquire_jobs_data()
    county_class = acquire_tx_county_class()
    hospital = create_hospital_count_df()
    covid = acquire_covid_cases()
    nursing = create_nursing_home_counts_df()
    tests = acquire_testing_totals()
    region_num = acquire_health_region_id()
    tx_svi = acquire_cdc_svi_index()
    beds = create_bed_count_df()

    # merge all dataframes into one big one
    df = food.join(income.set_index("FIPS"), on="FIPS")
    df1 = df.join(people.set_index("FIPS"), on="FIPS")
    df2 = df1.join(veteran.set_index("FIPS"), on="FIPS")
    df3 = df2.join(jobs.set_index("FIPS"), on="FIPS")
    df4 = df3.join(county_class.set_index("FIPS"), on="FIPS")
    df5 = df4.join(hospital.set_index("FIPS"), on="FIPS")
    df6 = df5.merge(covid, how="left", on="County")
    df7 = df6.merge(nursing, how="left", on="County")
    df8 = df7.merge(tests, how="left", on="County")
    df9 = df8.join(region_num.set_index("FIPS"), on="FIPS")
    df10 = df9.join(tx_svi.set_index("FIPS"), on="FIPS")
    df11 = df10.join(beds.set_index('FIPS'), on = 'FIPS')

    df = df11
    df = df.drop(columns = ['PopDensity2010', 'NumUnemployed2018', 'NumEmployed2018','Health Service Region (8)'])

    return df
